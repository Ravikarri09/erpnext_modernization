import json
import faiss
import numpy as np
from llm.ollama_embed import embed

MAX_CHUNKS = 100   # only process first 100 chunks

print("ğŸ“¦ Loading code chunks...")

with open("data/code_chunks.json", "r", encoding="utf-8") as f:
    chunks = json.load(f)

# Limit chunks for fast testing
chunks = chunks[:MAX_CHUNKS]

print("ğŸ”„ Generating embeddings for", len(chunks), "chunks (FAST MODE)...")

vectors = []

for i, chunk in enumerate(chunks):
    try:
        print(f"âš™ï¸ Embedding chunk {i+1}/{len(chunks)}")
        vec = embed(chunk["text"])
        vectors.append(vec)
    except Exception as e:
        print("âŒ Embedding failed:", e)

vectors = np.array(vectors).astype("float32")

print("ğŸ“ Vector shape:", vectors.shape)

if len(vectors) == 0:
    raise Exception("No embeddings generated. Check Ollama.")

dim = vectors.shape[1]
index = faiss.IndexFlatL2(dim)
index.add(vectors)

faiss.write_index(index, "vector_db/faiss.index")

print("âœ… Vector DB created with", len(vectors), "vectors (FAST MODE)")
